{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2950bd49",
   "metadata": {},
   "source": [
    "**Gerar vetor das rotas obtidas via Radares Automotivos**\n",
    "<br> O script acessa os arquivos gerados via radares automotivos; gera um modelo json para cada rota; carrega os arquivos json; requisita e armazena o valor do jobID de cada um dos arquivos json; requisita e faz o download dos resultados de cada jobID.\n",
    "<br>\n",
    "<br> O script divide-se em 3 etapas:\n",
    "<br>**ETAPA 1:** Define functions para serem usadas em cada arquivo\n",
    "<br>**ETAPA 2:** Gera os dados jobID em forma de lista (job_id)\n",
    "<br>**ETAPA 3:** Faz o download dos resultados\n",
    "<br>**ETAPA 4:** Descompacta e exclui os arquivos zip, mantendo os arquivos GIS e planilha\n",
    "<br>\n",
    "<br> obs: Este código foi tratado de uma versão de outra autoria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc305c",
   "metadata": {},
   "source": [
    "https://developer.tomtom.com/traffic-stats/traffic-stats-apis/route-analysis\n",
    "<br> Site com modelo JSON do API, com dica de como deve ficar a estrutura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77206bc",
   "metadata": {},
   "source": [
    "Dica de site para testar se o arquivo .geojson formado, é um arquivo JSON válido\n",
    "<br> https://jsonlint.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e6627",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ceb28",
   "metadata": {},
   "source": [
    "# 1 - DEFINIÇÃO DE FUNÇÕES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ce1a0",
   "metadata": {},
   "source": [
    "### Função: constructor(file)\n",
    "<br> Função para geração dos arquivos geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3f1f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Função para criar cosntructor e modelos Json para cada rota\n",
    "def constructor(file):\n",
    "    \n",
    "    #Lê arquivo do radar atuotomotivo e armazena id da viagem\n",
    "    rota = json.load(open(r\"local\"+str(file)))\n",
    "    viagem_id = rota[0][\"viagem_id\"]\n",
    "    print(\"rota: \"+str(file),\"\\n\"\n",
    "          \"viagem_id: \"+str(viagem_id),\"\\n\")\n",
    "    \n",
    "    # Cria uma pasta para cada viagem\n",
    "    try:\n",
    "        foldername = (r\"rota_blocos\\rota_\"+str(viagem_id))\n",
    "        #print(foldername)\n",
    "        os.mkdir(foldername)\n",
    "    except: \n",
    "        pass\n",
    "        #print(\"Pasta já Existente\")\n",
    "             \n",
    "    # Criaçao dos arquivos geojson para cada trecho da rota em análise\n",
    "    # Parte 1: Extração das datas e horários\n",
    "    data_obj_1 = rota[1][\"data_inicio\"]\n",
    "    data_inicio= datetime.datetime.strptime(data_obj_1, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    data_obj_2 = rota[len(rota)-1][\"data_final\"]\n",
    "    data_final= datetime.datetime.strptime(data_obj_2, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    #print (data_inicio.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    #print (data_final.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    \n",
    "    # Parte 2: Escreve o arquivo geojson\n",
    "    filename = \"rota_\"+str(viagem_id)+\".geojson\"\n",
    "    geojson_file = open(os.path.join(foldername, filename), 'w')\n",
    "    geojson_file.write('{\\n')\n",
    "    geojson_file.write('\"jobName\": \"rota_'+str(viagem_id)+'\",\\n')\n",
    "    geojson_file.write('\"distanceUnit\":\"KILOMETERS\",\\n')\n",
    "    geojson_file.write('\"routes\": [\\n{\\t\"name\":\"viagem_id_'+str(viagem_id)+'\",\\n')\n",
    "    geojson_file.write('\\t\"start\":{\"latitude\":'+str(rota[1][\"lat_inicio\"])+',\"longitude\":'+str(rota[1][\"long_inicio\"])+'},\\n')\n",
    "    for i in rota[2:len(rota)]:\n",
    "        geojson_file.write('\\t\"via\":[{\"latitude\":'+str(i[\"lat_inicio\"])+',\"longitude\":'+str(i[\"long_inicio\"])+'}],\\n')\n",
    "    geojson_file.write('\\t\"end\":{\"latitude\":'+str(rota[len(rota)-1][\"lat_final\"])+',\"longitude\":'+str(rota[len(rota)-1][\"long_final\"])+'},\\n')\n",
    "    geojson_file.write('\\t\"fullTransversal\":\"false\",\\n\\t\"zoneId\":\"UTC-3\",\\n\\t\"probeSource\":\"ALL\"}],\\n')\n",
    "    geojson_file.write('\"dateRanges\": [{\"name\":\"Via Radares SP\", \"from\": \"'+data_inicio.strftime(\"%Y-%m-%d\")+'\",\"to\": \"'+data_final.strftime(\"%Y-%m-%d\")+'\"}],\\n')  # Tive que escolher uma data manualmente, a dos radares não funcionou\n",
    "    geojson_file.write('\"timeSets\": [{\"name\":\"Via Radares SP\", \\n\"timeGroups\":[{\"days\":[\"'+data_inicio.strftime(\"%a\").upper()+'\"],\"times\":[\"'+data_inicio.strftime(\"%H:%M\")+'-'+data_final.strftime(\"%H:%M\")+'\"]}]}]\\n')  # dia da semana tem que ser em maíusculo e compatível com o dateRange\n",
    "    geojson_file.write('}')\n",
    "    geojson_file.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a95f15",
   "metadata": {},
   "source": [
    "### Função: BatchRequestJobID(batch_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555484c",
   "metadata": {},
   "source": [
    "Requisição (jobID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ca56d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def BatchRequestJobID(batch_key):\n",
    "    batch_id = []\n",
    "    global erro_json, erro_count\n",
    "    for key in tqdm(range(len(batch_list[batch_key]))):\n",
    "        try:\n",
    "            r = requests.post(url, json = batch_list[batch_key][key])\n",
    "            batch_id.append(r.json()['jobId'])\n",
    "        except:\n",
    "            print(\"Erro\")\n",
    "            print(batch_list[batch_key][key])\n",
    "            erro_json.append(batch_list[batch_key][key])\n",
    "    return print(batch_id, batch_list[batch_key][key][\"jobName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f09875",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def BatchRequestJobID(batch_key):\n",
    "    batch_id = {}\n",
    "    global erro_json, erro_count\n",
    "    for key in tqdm(range(len(batch_list[batch_key]))):\n",
    "        try:\n",
    "            r = requests.post(url, json = batch_list[batch_key][key])\n",
    "            batch_id[r.json()['jobId']]=batch_list[batch_key][key][\"jobName\"]\n",
    "        except:\n",
    "            print(\"Erro\")\n",
    "            print(batch_list[batch_key][key])\n",
    "            erro_json.append(batch_list[batch_key][key])\n",
    "    return batch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d80e3",
   "metadata": {},
   "source": [
    "### Função: baixar_arquivo(url, endereco)\n",
    "<br> Função para fazer download de cada arquivo segundo um jobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e26a13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Função para download dos arquivos\n",
    "def baixar_arquivo(url, endereco):\n",
    "    # faz requisição ao servidor\n",
    "    resposta = requests.get(url)\n",
    "    if resposta.status_code == requests.codes.OK:\n",
    "        with open(endereco, 'wb') as novo_arquivo:\n",
    "            novo_arquivo.write(resposta.content)\n",
    "        print(\"Donwload finalizado. Salvo em: {}\".format(endereco))\n",
    "    else:\n",
    "        resposta.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb4e7b",
   "metadata": {},
   "source": [
    "# 2 - GERAÇÃO DOS DADOS jobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add760d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cria pasta para armazenamento dos resultados\n",
    "try:\n",
    "    foldername_arquivos = r\"rota_blocos\"\n",
    "    os.mkdir(foldername_arquivos)\n",
    "except: \n",
    "    print(\"Pasta já Existente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa63912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cria arquivos .json\n",
    "arquivos = os.listdir(r\"local\") # Local dos arquivos gerados via API Radares_SP\n",
    "\n",
    "viagem_id = []\n",
    "for file in arquivos:\n",
    "    viagem_id.append(file.split(\"_\")[1].split(\".\")[0])\n",
    "# Aplicação da função Constructor\n",
    "    try:\n",
    "        viagem_id_fhand = constructor(file)        \n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c021ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carrega os arquivos .json\n",
    "geojson = []\n",
    "viagem_id_erro = []\n",
    "for i in viagem_id:\n",
    "    try:\n",
    "        f = open(foldername_arquivos+r\"\\rota_\"+str(i)+r\"\\rota_\"+str(i)+\".geojson\")\n",
    "        geojson.append(json.load(f))\n",
    "    except:\n",
    "        viagem_id_erro.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb8b67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Criação do batch\n",
    "batch_list = []\n",
    "batch = []\n",
    "for key in geojson:\n",
    "    batch.append(key)\n",
    "batch_list.append(batch.copy())\n",
    "print(len(batch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f1141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Requisição jobID\n",
    "\n",
    "# URL solicitação jobID\n",
    "url = 'https://api.tomtom.com/traffic/trafficstats/routeanalysis/1?key=<key>'\n",
    "erro_json = []\n",
    "for i in tqdm(range(len(batch_list))):\n",
    "    job_id = BatchRequestJobID(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba16b89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# job_id é um dicionário com os valores de jobID associados à rota em questão, para descobrir o valor do jobID de uma dada rota\n",
    "# aplicar o comando a seguir:\n",
    "#list(job_id.keys())[list(job_id.values()).index(\"rota_99435878\")]\n",
    "\n",
    "# Salvar job_id via arquivo PKL (savepoint) \n",
    "len(job_id)\n",
    "with open(os.path.join(foldername_arquivos,'jobID.pkl'), 'wb') as f:\n",
    "    pickle.dump([job_id], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d70cf1",
   "metadata": {},
   "source": [
    "# 3 - DOWNLOAD DOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar job_id\n",
    "try:\n",
    "    len(job_id)\n",
    "except:\n",
    "    job_id = pd.read_pickle(r\"local\\jobID.pkl\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b01219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com valores completos ou rejeitados\n",
    "try: completed_jobID = pd.read_pickle(r\"local\\completed_jobID.pkl\")[0]\n",
    "except: completed_jobID = []\n",
    "    \n",
    "try: rejected_jobID = pd.read_pickle(r\"local\\rejected_jobID.pkl\")[0]\n",
    "except: rejected_jobID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660f066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aplica a função de download, talvez seja necessário rodar mais de uma vez devido a demora nos cálculos dos resultados. Prestar\n",
    "# atenção na completed_jobID e comparar com a lista job_ID. A lista rejected_jobID apenas contém elementos que já foram\n",
    "# calculados e rejeitados.\n",
    "\n",
    "# Outro ponto importante, é que o bloco dá erro algumas vezes quando o status do jobID muda durante o acesso, caso aconteça,\n",
    "# apenas precisa rodar o bloco novamente, graças a lista completed_jobID, não será preciso refazer todos os downloads.\n",
    "\n",
    "for i in tqdm(job_id):\n",
    "    # Pula arquivos que já foram baixados\n",
    "    if i in completed_jobID or i in rejected_jobID: continue\n",
    "    \n",
    "    url = 'https://api.tomtom.com/traffic/trafficstats/status/1/'+i+'?key=<key>'\n",
    "    r = requests.get(url)    \n",
    "    # print(r.json(),\"\\n\")\n",
    "    \n",
    "    # Verifica o status do jobID, se é possível baixar ou não\n",
    "    try : \n",
    "        r.json()[\"urls\"]\n",
    "    except:\n",
    "        print(\"ERRO\",\"\\n\"\n",
    "             \"url:\",url,\"\\n\")\n",
    "        if r.json()[\"jobState\"] in (\"NEW\",\"CALCULATIONS\"):\n",
    "            print(\"Resultados ainda sendo calculados\", \"\\n\"\n",
    "                  \"jobState:\",r.json()[\"jobState\"],\"\\n\")\n",
    "        elif r.json()[\"jobState\"] == \"REJECTED\":\n",
    "            print(\"Viagem rejeitada \\\"Could not calculate routes\\\", confirmar.\", \"\\n\"\n",
    "                  \"jobState:\",r.json()[\"jobState\"],\"\\n\")\n",
    "                  #r.json()[\"messages\"])\n",
    "            rejected_jobID.append(i)\n",
    "        continue\n",
    "           \n",
    "    # Baixa arquivos        \n",
    "    if __name__ == \"__main__\":\n",
    "        BASE_URL = (r.json()['urls'][4])\n",
    "        OUTPUT_DIR = foldername_arquivos+r\"\\\\\"+job_id[i]  # Local à serem salvos os arquivos xlsx das rotas, lembrar de mudar\n",
    "        nome_arquivo = os.path.join(OUTPUT_DIR, job_id[i]+'.xlsx'.format(i))\n",
    "        baixar_arquivo(BASE_URL.format(i), nome_arquivo)\n",
    "    if __name__ == \"__main__\":\n",
    "        BASE_URL = (r.json()['urls'][3])\n",
    "        OUTPUT_DIR = foldername_arquivos+r\"\\\\\"+job_id[i] # Local à serem salvos os arquivos kmz das rotas, lembrar de mudar\n",
    "        nome_arquivo = os.path.join(OUTPUT_DIR, job_id[i]+\".zip\".format(i))\n",
    "        baixar_arquivo(BASE_URL.format(i), nome_arquivo)\n",
    "    if i not in completed_jobID: completed_jobID.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4fc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Salvar job_id via arquivo PKL (savepoint) \n",
    "with open(os.path.join(foldername_arquivos,'completed_jobID.pkl'), 'wb') as f:\n",
    "    pickle.dump([completed_jobID], f)\n",
    "\n",
    "with open(os.path.join(foldername_arquivos,'rejected_jobID.pkl'), 'wb') as f:\n",
    "    pickle.dump([rejected_jobID], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa18af",
   "metadata": {},
   "source": [
    "# 4 - Descompactar arquivos shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f258d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descompactar arquivos\n",
    "for i in completed_jobID:\n",
    "    # Confere se o arquivo final já existe\n",
    "    if os.path.isfile(foldername_arquivos+\"\\\\\"+str(job_id[i])+\"\\\\viagem_id_\"+job_id[i].split(\"_\")[1]+\"_1\\\\route.shp\") is True:\n",
    "        continue\n",
    "    # Descompacta e exclui o .zip    \n",
    "    try:\n",
    "        with zipfile.ZipFile(foldername_arquivos+\"\\\\\"+str(job_id[i])+\"\\\\\"+str(job_id[i])+\".zip\") as z:\n",
    "            z.extractall(path = foldername_arquivos+\"\\\\\"+str(job_id[i]),\n",
    "                         members = z.namelist()[0:4])\n",
    "        os.remove(foldername_arquivos+\"\\\\\"+str(job_id[i])+\"\\\\\"+str(job_id[i])+\".zip\") \n",
    "    # Caso nem o arquivo final (.shp) nem o zip seja encontrado, retorna a rota problemática para análise manual\n",
    "    except: \n",
    "        print(job_id[i])\n",
    "        continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclui pastas das rotas rejeitadas pelo API, ATENÇÃO PARA NÃO EXCLUIR PASTAS NECESSÁRIAS\n",
    "for i in rejected_jobID:\n",
    "    #print(foldername_arquivos+r\"\\rota_\"+str(job_id[i]))\n",
    "    shutil.rmtree(foldername_arquivos+\"\\\\\"+str(job_id[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
